{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Authors\n",
    "Özgür Aslan 2236958 aslan.ozgur@metu.edu.tr  \n",
    "Burak Bolat 2237097 burak.bolat@metu.edu.tr\n",
    "\n",
    "## Paper Authors\n",
    "\n",
    "Mahmoud Afifi, Marcus A. Brubaker, Michael S. Brown\n",
    "\n",
    "## Paper Information Summary\n",
    "The paper we selected to implement is [HistoGAN: Controlling Colors of GAN-Generated and Real Images via Color Histograms](https://arxiv.org/abs/2011.11731)\n",
    "The main idea of the paper is to use color histogram of target images to control the colors of the generated image without changing the high level features of the generated image (gender, having glasses, beard, hair style and objects in the background...)  \n",
    "To accomplish this idea, they modify the StyleGAN2 architecture:\n",
    "- In the last 2 style blocks, instead of using affine transformation of the w vector, they use the color histogram projected by a neural network.\n",
    "- In addition to mixing, path length and r1 regularizations, they use a histogram based loss.\n",
    "- Two different target images are used to obtain a interpolated histogram. This histogram is used by the generator network to control the generated image's color scheme. \n",
    "- Interpolation of histograms is used to prevent the generator network to overfit color histograms of the training dataset.\n",
    "- Due to hardware limitations, image resolution of 256 is used.\n",
    "- Also due to hardware limitations they use batches of size 2 with gradient accumulation.\n",
    "\n",
    "### Histogram Usage with the SytleGAN2 Architecture \n",
    "![arch](materials/arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Goals\n",
    "This study implements only the HistoGAN part of the paper, not ReHistoGAN.\n",
    "Our experimental result goal is to train HistoGAN to generate Anime Faces with controlled histograms and compute the FID scores given in the paper.\n",
    "\n",
    "![exp1](materials/expg1.jpeg)\n",
    "![exp2](materials/expg2.jpeg)\n",
    "\n",
    "### Histogram Computation\n",
    "Computing Histogram is critical since it directly affects style of last 2 blocks. Authors used chrominance logarithm space. It normalizes each color channel with respect to other two channels in logarithmic space. In this chrominance space, there is u and v axes. That is, if we look at red channel's chrominance space, u is the normalization of red channel with respect to green and v is the normalization of red channel with respect to blue. Same holds for all color channels.  \n",
    "\n",
    "After shifting RGB space to RGB-uv space, the histogram is computed as it is computationally efficient and more stable. Authors used 64 bin for the histogram which results in 64x64 histogram for u and v channel. We have 3 channels, namely red, green and blue, thus, overall the histogram is 3x64x64. Histogram is weighted with respect to pixel intesity, i.e. if a pixel has high RGB values its affect on the histogram bin is higher. Last difference of the histogram than histograms of previous works is kernels for computing bins. Authors do not used exact bin selection. Instead, they put a normalized pixel into a bin with respect to soft kernel. That means, if we have a red channel after normalized with respect to green and blue, we have some u and v values. Instead of just adding 1 (1 being chosen for simplicty, remember intensity multiplication) to the bin of H(u,v), they add values to the neighbour of (u,v) with the value after inverse quadratic kernel.\n",
    "\n",
    "Histogram feature is computed like syle vector (w). It passed through the same neural network architecture with different parametes. More precisely, histogram passes through 8 layer MLP and outputs latent histogram vector size of 512.  \n",
    "\n",
    "We put some computed histograms by us. Images taken from internet crawling.  \n",
    "\n",
    "![asd](materials/gresized.png) ![asc](materials/ghist1.png)  \n",
    "![asb](materials/rresized.png) ![asj](materials/rhist1.png)\n",
    "\n",
    "### Loss for Training with Histogram\n",
    "Since the paper uses target histogram for generation, generated image should have close histogram to target. Thus a closeness measure Hellinger distance between histogram of generated and target images is computed and tried to minimize. You can check the losses belove.\n",
    "\n",
    "Difference between histograms  \n",
    "![l1](materials/hloss.png)\n",
    "\n",
    "Total loss for generator  \n",
    "![lt](materials/total_loss.png)\n",
    "\n",
    "### Discriminator\n",
    "\n",
    "Discriminator consist of residual blocks. There are log_2(N)-1 such bloks where N is image resolution, to be spesific 256. As a result, the discriminator has 7 layers. First block takes 3 channel image as input and outputs m channel features. After the first block, each block produces 2*m of previous block. At the end of residual blocks, a FC layer outputs a scaler.\n",
    "\n",
    "![res](materials/residual.png)\n",
    "\n",
    "\n",
    "### Important Note on Dataset\n",
    "The Anime Face Dataset is a Kaggle dataset, thus, requires a Kaggle account. Using an account one can download it from:\n",
    "https://www.kaggle.com/datasets/splcher/animefacedataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faced Challenges\n",
    "\n",
    "#### Architecture Challenges\n",
    "\n",
    "As we stated, HistoGAN is built on StyleGAN2. StyleGAN2 scales directly the weigth of the model, unlike the first version does the nearly same operations, namely mod-demod, on convolved image (or say not directly weights for convoltion filters). The original StyleGAN2 implemented using Tensorflow which allows to multiplication on weights, that is called in-place operation on variables. However, Pytorch does not allow in-place operations on built in modules like torch.nn.Conv2d. Therefore, we implemented a conv2d version. Model parameters are Pytorch Variables and convolution operation is handled with fold and unfold operations of Pytorch. Doing so we can apply convolution after scaling weights of convolution filters.\n",
    "\n",
    "\n",
    "#### Training Challenges\n",
    "\n",
    "During the training phase, the paper does not mentioned how generator outputs the images. We made different assumption such as using sigmoid or tanh to generate pixels in a range. Another assumption for the same problem is using ReLU or leaky ReLU that we saw from other generator implementation.  \n",
    "StyleGAN2 stated that they used non saturating loss for some datasets and WGAN-GP loss for other datasets. HistoGAN paper does not clearly mention on this. Consequently, we implemented both but non saturating loss lead numerical issues like nan or infs. On the other hand, WGAN-GP computes high loss values and results in rapid saturation (see above figures). This issues may be resulted from hand implemented convolution operations. \n",
    "\n",
    "#### Ambigious Loss\n",
    "\n",
    "The paper does not state clearly which loss functions for both generator and discriminator is used. It states how one can combine with histogram loss, but, exact losses are not clear. As a result, we accounted three discriminator losses. We focused mostly non saturating losses since it was mentioned in the paper. \n",
    "\n",
    "We implemented gradient penalty and R1 penalty for Discriminator. R1 penalty is the gradient penalty for only scoring the real images. Seeing both penalty does not reach good performance, we trained the Discriminator with Spectral Normalization. \n",
    "\n",
    "#### Model Initialization  \n",
    "\n",
    "The model initalization was unclear. StyleGAN2 initializes weights with random normal and uses equalized learning rate method, however, initalization with kaiming may also be used. Authors granted the arhitecture from StyleGAN2, yet, they do not fully copy the training details. As a consequence, they do not mentioned that they used equalized learning rate. We implemented kaiming normal implementation and equalized learning rate, however, there was no such important improvements. As a result, we stick with kaiming normal.\n",
    "\n",
    "#### Training with Different Architectures\n",
    "\n",
    "Since we can not reach the desired generated images, we implemented StyleGANv1 + HistoGAN Block and a slight changed version of StyleGANv1 which was mentioned in StyleGAN2 paper (please see revisede StyleGAN in StyleGAN2 paper). We implemented all versions given at the figure below.\n",
    "\n",
    "![archs](materials/archs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation Info\n",
    "- python 3.7.13\n",
    "- pytorch 1.11.0 with cuda10.2   \n",
    "We used conda environments for clean library setups and included environment.yml file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    num_epochs = 10, # number of epochs for training\n",
    "    batch_size = 16, # batch size\n",
    "    acc_gradient_total = 16, # total number of samples seen by the networks in 1 iteration\n",
    "    r1_factor = 10, # coefficient of the r1 regularization term\n",
    "    r1_update_iter = 4, # in every r1_update_iter r1 regularization is used\n",
    "    decay_coeff = 0.99, # ema decay coefficient for updating the path length target varaible\n",
    "    plr_update_iter = 32, # in every plr_update_iter the path length regularization is used\n",
    "    save_iter = 1200, # in every save_iter the images are saved\n",
    "    image_res = 64, # the resolution of the images\n",
    "    network_capacity = 16, # capacity of the network used for channels of constant input in generator \n",
    "    latent_dim = 512, # dimensionalty of the noises\n",
    "    bin_size = 64, # bin size of the histograms\n",
    "    learning_rate = 0.0002, # learning rate\n",
    "    mapping_layer_num = 8, # number of Linear layers in Mapping part of the Generator (z -> w)\n",
    "    mixing_prob = 0.9, # probality of using two distinct noises for generation\n",
    "    use_plr = True, # Wheter to use path length reg in training\n",
    "    use_r1r = True, # Wheter to use r1 reg in training\n",
    "    kaiming_init = True, # Initiazlize networks with kaiming initialization method by He et al.\n",
    "    use_eqlr = False, # use eqularized learning coefficients for weights (similar to kaiming but used in every forward calculation)\n",
    "    use_spec_norm = False, # use spectral normalization of Discriminator weights (For stabilization)\n",
    "    disc_arch= \"ResBlock\", # architecture of the Discriminator (used for bookkeeping)\n",
    "    gen_arch = \"InputModDemod\", # architecture of the Generator (used for bookkeeping)\n",
    "    optim=\"Adam\", # Optimizer used (Adam or DiffGrad)\n",
    "    optim_params = (0.5, 0.9),  # Optimizer beta values (Adam and DiffGrad)\n",
    "    loss_type=\"wasser\", # Loss type to use (Wasserstein, Hinge, Log Sigmoid)\n",
    "    save_model_path = \".\", # Path to save generator and discriminator\n",
    "    pre_gen_path = None, # for loading a pretrained network\n",
    "    pre_disc_path = None, # for loading a pretrained network\n",
    "    training_dataset_path = \"images/anime_face\", # path of training images \n",
    "    generated_images_path = \"generated_images\", # path to save generated images\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n",
      "Networks Created\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://pytorch.org/vision/stable/auto_examples/plot_visualization_utils.html\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "\n",
    "\n",
    "def show(imgs):\n",
    "    imgs = imgs.mul(255).add_(0.5).clamp_(0, 255).to(\"cpu\", torch.uint8)\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14511/1608251864.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtotal_iter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mhist_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/HistoGANN/trainer.py\u001b[0m in \u001b[0;36mtrain_discriminator\u001b[0;34m(self, chunk_data, iter)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mhist_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_hist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmixing_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_gen_layers\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixing_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mfake_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_hist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0mfake_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfake_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mfake_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/histo/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/HistoGANN/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z, target_hist, test)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_w_from_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0mrgb_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_image_from_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrgb_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/HistoGANN/model.py\u001b[0m in \u001b[0;36mgen_image_from_w\u001b[0;34m(self, w, target_hist, test)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mfm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearned_const_inp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstylegan2_block\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstylegan2_blocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m             \u001b[0mfm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstylegan2_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m             \u001b[0mfm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/histo/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/HistoGANN/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, fm, w, test)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mnoise_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0mnoise_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mstyle_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Traning loop with gradient accumulation\n",
    "total_iter = 0\n",
    "for _ in range(0, trainer.num_epochs):\n",
    "    for iter, chunk_data in enumerate(trainer.dataloader):\n",
    "        total_iter += 1\n",
    "        hist_list = trainer.train_discriminator(chunk_data, iter)\n",
    "        trainer.train_generator(hist_list, iter)\n",
    "\n",
    "        # save iamges after every save_iter\n",
    "        if iter % trainer.save_iter == 0:\n",
    "            z = torch.randn(trainer.batch_size, trainer.num_gen_layers,trainer.latent_dim).to(trainer.device)\n",
    "            fake_data, _ = trainer.generator(z, hist_list[0])\n",
    "            fd = [data for data in fake_data]\n",
    "            grid = make_grid(fd, nrow=trainer.batch_size, normalize=True)\n",
    "            show(grid)\n",
    "            plt.show()\n",
    "            del grid, fd, fake_data, z\n",
    "            torch.save(trainer.generator.state_dict(), os.path.join(trainer.save_model_path, \"generator.pt\"))\n",
    "            torch.save(trainer.discriminator.state_dict(), os.path.join(trainer.save_model_path, \"discriminator.pt\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eval\n",
    "\n",
    "generator = HistoGAN(network_capacity, latent_dim, bin_size, image_res, mapping_layer_num, kaiming_init=kaiming_init, use_eqlr=use_eqlr)\n",
    "generator_dir = \"models/generator_1.pt\"\n",
    "generator.load_state_dict(torch.load(generator_dir))\n",
    "print(\"Model loaded\")\n",
    "fid_batch_scores = eval.fid_scores(generator, \"images\")\n",
    "fid_np = np.array(fid_batch_scores)\n",
    "print(fid_np.mean())\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0f6dd8c1d7576cf0de48aad06b0e935e2de8273eecf0f787202423f6355e44ee"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
